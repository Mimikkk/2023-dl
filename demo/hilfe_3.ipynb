{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7413224,
     "sourceType": "datasetVersion",
     "datasetId": 4312186
    },
    {
     "sourceId": 7415792,
     "sourceType": "datasetVersion",
     "datasetId": 4314013
    }
   ],
   "dockerImageVersionId": 30636,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.utils import show_images\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from typing import Callable\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "from typing import Optional\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-16T21:39:57.406214Z",
     "iopub.execute_input": "2024-01-16T21:39:57.406576Z",
     "iopub.status.idle": "2024-01-16T21:40:00.868422Z",
     "shell.execute_reply.started": "2024-01-16T21:39:57.406545Z",
     "shell.execute_reply": "2024-01-16T21:40:00.867525Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "manual_seed = 999\n",
    "print(f\"Random Seed: {manual_seed}\")\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "latent_vector_size = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-16T21:40:07.516636Z",
     "iopub.execute_input": "2024-01-16T21:40:07.517290Z",
     "iopub.status.idle": "2024-01-16T21:40:07.524022Z",
     "shell.execute_reply.started": "2024-01-16T21:40:07.517257Z",
     "shell.execute_reply": "2024-01-16T21:40:07.523065Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%autoreload 2\n",
    "\n",
    "from src.mod.plugins.early_stopping import EarlyStopping\n",
    "from src.mod.datasets import CelebA\n",
    "from src.mod.models import Classifier, Generator, Discriminator\n",
    "from src.mod.datasets.utils import split_dataset\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "from typing import Any\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize(64),\n",
    "  transforms.CenterCrop(64),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = CelebA(\n",
    "  dataset_path='../resources/datasets/celeba',\n",
    "  image_directory='img_align_celeba',\n",
    "  annotations_directory='annotations',\n",
    "  image_transform=transform\n",
    ")\n",
    "\n",
    "generator = Generator(latent_vector_size, 64, 3, with_weights=torch.load('generator.pt'))\n",
    "discriminator = Discriminator(3, 64, with_weights=torch.load('discriminator.pt'))\n",
    "classifier = Classifier(64, 3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_examples():\n",
    "  dataloader = DataLoader(dataset, batch_size=64, num_workers=2)\n",
    "  (images, _, _) = next(iter(dataloader))\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(\"Training Images\")\n",
    "  show_images(images)\n",
    "\n",
    "def show_annotated_example():\n",
    "  dataloader = DataLoader(dataset, batch_size=1, num_workers=2)\n",
    "  (images, annotations, _) = next(iter(dataloader))\n",
    "  annotations = annotations[0][0]\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.axis(\"off\")\n",
    "  plt.title(\"Example Image\")\n",
    "  show_images(images)\n",
    "  for (i, column) in enumerate(dataset.annotations.columns):\n",
    "    print(f\"{i + 1:>2}: {annotations[i] > 0.9 and 'Yes' or 'No':>3} - {column}\")\n",
    "\n",
    "show_examples()\n",
    "show_annotated_example()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Process:\n",
    "1. GAN Training:\n",
    "- Train D to differentiate real images from those generated by G.\n",
    "- Train G to fool D into classifying its outputs as real images.\n",
    "2. Classifier Training:\n",
    "- Train C using real images and their trait annotations.\n",
    "3. Combined Training:\n",
    "- Train G to not only fool D but also to generate images that C classifies with specified traits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.mod.datasets.utils import create_latent_vectors\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train_gan(\n",
    "    generator: nn.Module,\n",
    "    discriminator: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    *,\n",
    "    epoch_count: int,\n",
    "    save_to: Optional[str] = None,\n",
    "    device: torch.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "  generator.train()\n",
    "  discriminator.train()\n",
    "  generator.to(device)\n",
    "  discriminator.to(device)\n",
    "\n",
    "  RealLabel = 1.0\n",
    "  FakeLabel = 0.0\n",
    "\n",
    "  lr = 0.0002\n",
    "  beta1 = 0.5\n",
    "  \n",
    "  discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "  discriminator_scheduler = ReduceLROnPlateau(discriminator_optimizer, 'min', patience=20, factor=0.5)\n",
    "  generator_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "  generator_scheduler = ReduceLROnPlateau(generator_optimizer, 'min', patience=20, factor=0.5)\n",
    "\n",
    "  generator_criterion = nn.BCELoss()\n",
    "  discriminator_criterion = nn.BCELoss()\n",
    "  fixed_noise = create_latent_vectors(64, latent_vector_size, device)\n",
    "\n",
    "  generator_losses = []\n",
    "  discriminator_losses = []\n",
    "  iterations = 0\n",
    "\n",
    "  for epoch in tqdm(range(1, epoch_count + 1), desc=\"Epochs\", total=epoch_count):\n",
    "    for batch_nr, (images, _, _) in tqdm(enumerate(dataloader, 0), desc=\"Batches\", total=len(dataloader)):\n",
    "      discriminator.zero_grad()\n",
    "      real_images = images.to(device)\n",
    "      batch_size = real_images.size(0)\n",
    "\n",
    "      real_labeling = torch.full((batch_size,), RealLabel, dtype=torch.float, device=device)\n",
    "      expected_labeling = discriminator(real_images).view(-1)\n",
    "      real_images_discriminator_loss = discriminator_criterion(expected_labeling, real_labeling)\n",
    "      real_images_discriminator_loss.backward()\n",
    "\n",
    "      latent_vectors = create_latent_vectors(batch_size, latent_vector_size, device)\n",
    "      fake_images = generator(latent_vectors)\n",
    "      real_labeling.fill_(FakeLabel)\n",
    "      expected_labeling = discriminator(fake_images.detach()).view(-1)\n",
    "\n",
    "      fake_images_discriminator_loss = discriminator_criterion(expected_labeling, real_labeling)\n",
    "      fake_images_discriminator_loss.backward()\n",
    "\n",
    "      discriminator_loss = real_images_discriminator_loss + fake_images_discriminator_loss\n",
    "      discriminator_scheduler.step(discriminator_loss)\n",
    "\n",
    "      generator.zero_grad()\n",
    "      real_labeling.fill_(RealLabel)\n",
    "      expected_labeling = discriminator(fake_images).view(-1)\n",
    "      generator_loss = generator_criterion(expected_labeling, real_labeling)\n",
    "      generator_loss.backward()\n",
    "      generator_scheduler.step(generator_loss)\n",
    "\n",
    "      if batch_nr % 50 == 0:\n",
    "        dloss = discriminator_loss.item()\n",
    "        gloss = generator_loss.item()\n",
    "        print(f'[{epoch}/{epoch_count}][{batch_nr + 1}/{len(dataloader)}]', f\"DLoss: {dloss:.4f}\", f\"Gloss: {gloss:.4f}\")\n",
    "\n",
    "      generator_losses.append(generator_loss.item())\n",
    "      discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "      if (iterations % 500 == 0) or ((epoch == epoch_count - 1) and (batch_nr == len(dataloader) - 1)):\n",
    "        with torch.no_grad():\n",
    "          fake_images = generator(fixed_noise).detach().cpu()\n",
    "        show_images(fake_images)\n",
    "\n",
    "      iterations += 1\n",
    "  generator.eval()\n",
    "  discriminator.eval()\n",
    "\n",
    "  if save_to:\n",
    "    torch.save(generator.state_dict(), f\"g.{save_to}\")\n",
    "    torch.save(discriminator.state_dict(), f\"d.{save_to}\")\n",
    "\n",
    "  return generator, discriminator\n",
    "\n",
    "dataset = CelebA(\n",
    "  dataset_path='../resources/datasets/celeba',\n",
    "  image_directory='img_align_celeba',\n",
    "  annotations_directory='annotations',\n",
    "  max_image_count=5000,\n",
    "  image_transform=transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, num_workers=8)\n",
    "\n",
    "generator = Generator(latent_vector_size, 64, 3, with_weights=torch.load('g.checkpoint.pt'))\n",
    "discriminator = Discriminator(3, 64, with_weights=torch.load('d.checkpoint.pt'))\n",
    "\n",
    "# train_gan(\n",
    "#   generator,\n",
    "#   discriminator,\n",
    "#   dataloader,\n",
    "#   epoch_count=20,\n",
    "#   save_to=\"checkpoint.pt\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train_latent_vector_generator(\n",
    "    generator: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    epoch_count: int,\n",
    "    save_to: str,\n",
    "    on_train: Optional[Callable[[int, float, Tensor, Tensor, Tensor, list[str]], None]] = None,\n",
    "    on_validate: Optional[Callable[[int, float, Tensor, Tensor, Tensor, list[str]], None]] = None,\n",
    "    on_epoch: Optional[Callable[[int, float, float], None]] = None\n",
    "):\n",
    "  generator.train()\n",
    "\n",
    "  optimizer = AdamW(generator.parameters())\n",
    "  scheduler = ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.5)\n",
    "  early_stopping = EarlyStopping(\n",
    "    generator,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    save_to=save_to\n",
    "  )\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  for epoch in range(1, epoch_count + 1):\n",
    "    generator.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_nr, (images, _, paths) in tqdm(\n",
    "        enumerate(train_dataloader, 1),\n",
    "        desc=\"Training\",\n",
    "        total=len(train_dataloader)\n",
    "    ):\n",
    "      images = images.to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      latent_vectors = create_latent_vectors(images.size(0), latent_vector_size, device)\n",
    "      generated = generator(latent_vectors)\n",
    "      loss = criterion(generated, images)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step(loss)\n",
    "\n",
    "      train_loss += loss.item()\n",
    "      if on_train: on_train(batch_nr, loss, latent_vectors, generated, images, paths)\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    generator.eval()\n",
    "    val_loss = 0.0\n",
    "    for batch_nr, (images, _, paths) in tqdm(\n",
    "        enumerate(val_dataloader, 1),\n",
    "        desc=\"Validation\",\n",
    "        total=len(val_dataloader)\n",
    "    ):\n",
    "      images = images.to(device)\n",
    "      latent_vectors = create_latent_vectors(images.size(0), latent_vector_size, device)\n",
    "      generated = generator(latent_vectors)\n",
    "      loss = criterion(generated, images)\n",
    "      val_loss += loss.item()\n",
    "      if on_validate: on_validate(batch_nr, loss, latent_vectors, generated, images, paths)\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    if early_stopping.step(val_loss): break\n",
    "    if on_epoch: on_epoch(epoch, train_loss, val_loss)\n",
    "  generator.eval()\n",
    "  return generator\n",
    "\n",
    "dataframe: pd.DataFrame = pd.DataFrame()\n",
    "generator = Generator(latent_vector_size, 64, 3, with_weights=torch.load('generator.pt'))\n",
    "generator.to(device)\n",
    "train_dataloader, validation_dataloader = split_dataset(dataset, 0.8)\n",
    "\n",
    "\n",
    "# every epoch show generated images grid\n",
    "def on_epoch(epoch, train_loss, val_loss):\n",
    "  print(f\"Epoch {epoch}: Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}\")\n",
    "  vec = create_latent_vectors(64, latent_vector_size, device)\n",
    "  generated = generator(vec)\n",
    "  show_images(generated)\n",
    "\n",
    "train_latent_vector_generator(\n",
    "  generator,\n",
    "  train_dataloader,\n",
    "  validation_dataloader,\n",
    "  epoch_count=20,\n",
    "  save_to=\"generator_checkpoint.pt\",\n",
    "  on_epoch=on_epoch\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create latent vectors for all images in the dataset by running the generator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vec = create_latent_vectors(1, latent_vector_size, device)\n",
    "generated = generator(vec)\n",
    "\n",
    "show_images(generated)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataframe"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vector = (\n",
    "  torch.tensor(pd.to_numeric(dataframe.loc[0][0:100]).to_numpy(), device=device)\n",
    "  .unsqueeze(1).unsqueeze(1).unsqueeze(0).float()\n",
    ")\n",
    "path = dataframe.loc[0]['img']\n",
    "\n",
    "real_image = Image.open(f\"../resources/datasets/celeba/img_align_celeba/{path}\").convert(\"RGB\")\n",
    "real_image = transform(real_image).unsqueeze(0).to(device)\n",
    "\n",
    "show_images(generator(vector), real_image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "latent_vector = torch.randn(64, latent_vector_size, 1, 1, device=device, requires_grad=True)\n",
    "generated = generator(latent_vector)\n",
    "\n",
    "show_images(generated)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe.to_csv('latent_vectors.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('latent_vectors.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read annotations from celeba dataset\n",
    "annotations = pd.read_csv('../resources/datasets/celeba/list_attr_celeba.txt', skiprows=1, delim_whitespace=True, index_col=None)\n",
    "annotations.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test classifier on latent space from the dataset and show real image and predicted attributes\n",
    "(vector, attributes) = classifier_dataset[0]\n",
    "vector = torch.tensor(vector).to(device)\n",
    "image = Image.open(f\"../resources/datasets/celeba/img_align_celeba/000001.jpg\").convert(\"RGB\")\n",
    "classified = classifier(vector).cpu().detach().numpy()\n",
    "error = np.abs(classified - attributes)\n",
    "\n",
    "for i, err in enumerate(error[0]):\n",
    "  print(f\"{i:>2}: {err:.2f} - {annotations.columns[i]}\")\n",
    "  plt.imshow(image)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latent_vector = torch.randn(1, latent_vector_size, 1, 1, device=device, requires_grad=True)\n",
    "\n",
    "generated = generator(latent_vector)\n",
    "classified = classifier(latent_vector.resize(1, latent_vector_size)).cpu().detach().numpy()\n",
    "\n",
    "for i, err in enumerate(classified[0]):\n",
    "  print(f\"{i:>2}: {err:.2f} - {annotations.columns[i]}\")\n",
    "show_images(generated)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_attribute_direction(latent_vectors, classifier, target_scores):\n",
    "  latent_vectors = latent_vectors.requires_grad_()\n",
    "  attribute_scores = classifier(latent_vectors)\n",
    "  loss = torch.nn.functional.mse_loss(attribute_scores, target_scores)\n",
    "  loss.backward()\n",
    "  print(latent_vectors.grad)\n",
    "  attribute_direction = latent_vectors.grad\n",
    "  return attribute_direction.mean(dim=0)\n",
    "\n",
    "def steer_away_from_attribute(latent_vector, direction, intensity=1.0):\n",
    "  return latent_vector - intensity * direction\n",
    "\n",
    "latent_vectors = torch.randn(32, latent_vector_size).to(device)\n",
    "target_scores = torch.ones(32, 40).to(device)\n",
    "\n",
    "latent_vectors = latent_vectors.resize(32, latent_vector_size)\n",
    "direction = find_attribute_direction(latent_vectors, classifier, target_scores)\n",
    "adjusted_latent_vector = steer_away_from_attribute(latent_vectors, direction, intensity=1)\n",
    "\n",
    "real_images = generator(latent_vectors.resize(32, latent_vector_size, 1, 1))\n",
    "move_images = generator(adjusted_latent_vector.resize(32, latent_vector_size, 1, 1))\n",
    "\n",
    "show_images(real_images, move_images)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
